# this script ranks the explanations generated by three different pipelines

import pandas as pd
import os
import json
import re
import time
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

PROMPT = """

Evaluate the QUALITY of the explanations below, according to the ILORA Evaluation Framework.
For each criterion, give a score from 1 to 5 (1 = lowest quality, 5 = highest quality).

CRITERIA:

1. Informativeness (I) - Does the explanation provide new information, such as background knowledge or additional context that helps understand the decision?

2. Logicality (L) - Does the explanation follow a reasonable thought process? Is there a strong causal relationship between the explanation and the result?

3. Objectivity (O) - Is the explanation objective and free from excessive subjective emotion or bias?

4. Readability (R) - Does the explanation follow proper grammatical and structural rules? Are the sentences coherent and easy to understand?

5. Accuracy (A) - Does the generated explanation align with the actual label? Does the explanation accurately reflect the result?

### Input
**Claim:** {claim}

**Baseline**
- Label: {baseline_label}
- Justification: {baseline_justification}

**RAG**
- Label: {rag_label}
- Justification: {rag_justification}

**GraphRAG**
- Label: {graphrag_label}
- Justification: {graphrag_justification}

### Task
Rank them from best to worst based on the ILORA CRITERIA.

### Example Output Format
```json
{{"ranking": ["<your_first_pipeline_choice>", "<your_second_pipeline_choice>", "<your_third_pipeline_choice>"], "explanation": "<your explanation here>"}}
```

### Your Response (JSON only, no other text):
```json
"""


def normalize_claim(claim: str) -> str:
    return str(claim).lower().strip()


def load_bad_claims(bad_claims_path):
    if not os.path.exists(bad_claims_path):
        print(f"⚠️ Bad claims file not found: {bad_claims_path}")
        return set()

    bad_df = pd.read_csv(bad_claims_path)
    if "claim" not in bad_df.columns:
        print(f"⚠️ 'claim' column not found in bad claims file")
        return set()

    bad_df["claim_norm"] = bad_df["claim"].apply(normalize_claim)
    bad_claims_set = set(bad_df["claim_norm"].tolist())

    print(f"✅ Loaded {len(bad_claims_set)} bad claims\n")
    return bad_claims_set


MODEL_ID = "prometheus-eval/prometheus-7b-v2.0"

print("Loading Prometheus 7B model...", flush=True)
prom_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
if prom_tokenizer.pad_token is None:
    prom_tokenizer.pad_token = prom_tokenizer.eos_token
prom_model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID, device_map="auto", torch_dtype=torch.float16
)
print("Model loaded successfully!\n", flush=True)


def call_prometheus(prompt: str, max_new_tokens: int = 256, temp: float = 0.0):
    inputs = prom_tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
    input_ids = inputs["input_ids"].to(prom_model.device)
    attention_mask = inputs["attention_mask"].to(prom_model.device)

    with torch.no_grad():
        outputs = prom_model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            temperature=temp if temp > 0 else None,
            do_sample=temp > 0,
            top_p=1.0 if temp > 0 else None,
            top_k=0 if temp > 0 else None,
            pad_token_id=prom_tokenizer.eos_token_id,
        )

    out_ids = outputs[0][len(input_ids[0]) :]
    response = prom_tokenizer.decode(out_ids, skip_special_tokens=True)

    if "Your Response (JSON only, no other text):" in response:
        response = response.split("Your Response (JSON only, no other text):")[
            -1
        ].strip()

    return response


def parse_prometheus_json(response, claim_idx, error_log_path):

    try:

        start_idx = response.find("{")
        end_idx = response.rfind("}") + 1

        if start_idx == -1 or end_idx <= start_idx:
            raise ValueError("No JSON object found in response")

        json_str = response[start_idx:end_idx]
        result = json.loads(json_str)

        if "ranking" not in result:
            raise ValueError("Missing required fields in JSON")

        return result

    except Exception as e:

        with open(error_log_path, "a", encoding="utf-8") as f:
            f.write(f"\n{'='*70}\n")
            f.write(f"CLAIM: {claim_idx}\n")
            f.write(f"TIMESTAMP: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ERROR: {str(e)}\n")
            f.write(f"{'='*70}\n")
            f.write(f"MODEL OUTPUT:\n{response}\n")
            f.write(f"{'='*70}\n\n")

        return None


def process_claim_with_retry(
    full_prompt, claim_text, error_log_path, max_retries=2, retry_delay=2
):
    last_response = None

    for attempt in range(1, max_retries + 1):
        try:

            response = call_prometheus(full_prompt, max_new_tokens=512, temp=0.001)
            last_response = response

            result = parse_prometheus_json(response, claim_text[:50], error_log_path)

            if result is not None:

                if attempt > 1:
                    print(f"  ✓ Succeeded on attempt {attempt}", flush=True)
                return result, response
            else:

                if attempt < max_retries:
                    print(
                        f"Attempt {attempt}/{max_retries} failed (parsing error). Retrying in {retry_delay}s...",
                        flush=True,
                    )
                    time.sleep(retry_delay)
                else:
                    print(
                        f"All {max_retries} attempts failed (parsing error)", flush=True
                    )
                    return None, last_response

        except Exception as e:

            if attempt < max_retries:
                print(
                    f"Attempt {attempt}/{max_retries} failed ({str(e)[:50]}). Retrying in {retry_delay}s...",
                    flush=True,
                )
                time.sleep(retry_delay)
            else:
                print(f"All {max_retries} attempts failed: {str(e)[:100]}", flush=True)
                return None, last_response

    return None, last_response


def load_processed_claims(save_path):

    if os.path.exists(save_path):
        try:
            df = pd.read_csv(save_path)
            if "claim" in df.columns:

                successful = set(df[df["rank_1st"].notna()]["claim"].dropna().unique())

                failed = set(df[df["rank_1st"].isna()]["claim"].dropna().unique())
                return successful, failed, len(df)
        except Exception as e:
            print(f"  ⚠️  Error reading {save_path}: {e}", flush=True)
    return set(), set(), 0


def update_failed_claim_in_csv(save_path, claim, result, raw_response):

    df = pd.read_csv(save_path)

    mask = df["claim"] == claim

    if result is not None:

        df.loc[mask, "rank_1st"] = (
            result["ranking"][0] if len(result["ranking"]) > 0 else None
        )
        df.loc[mask, "rank_2nd"] = (
            result["ranking"][1] if len(result["ranking"]) > 1 else None
        )
        df.loc[mask, "rank_3rd"] = (
            result["ranking"][2] if len(result["ranking"]) > 2 else None
        )
        df.loc[mask, "explanation"] = result.get("explanation", "")
        df.loc[mask, "processing_failed"] = False
        df.loc[mask, "raw_model_output"] = raw_response
    else:

        df.loc[mask, "processing_failed"] = True
        df.loc[mask, "raw_model_output"] = raw_response

    df.to_csv(save_path, index=False)


# file mapping for different experimental settings
# file paths should be updated accordingly
# the columns in the data files are assumed to be: claim, predicted_label, predicted_justification, label(ground truth)
file_mapping = {
    "mixed_few_shot": {
        "baseline": "path_to_mixed_few_shot_baseline_results.csv",
        "rag": "path_to_mixed_few_shot_rag_results.csv",
        "graphrag": "path_to_mixed_few_shot_graphrag_results.csv",
    },
    "mixed_zero_shot": {
        "baseline": "path_to_mixed_zero_shot_baseline_results.csv",
        "rag": "path_to_mixed_zero_shot_rag_results.csv",
        "graphrag": "path_to_mixed_zero_shot_graphrag_results.csv",
    },
    "small_few_shot": {
        "baseline": "path_to_small_few_shot_baseline_results.csv",
        "rag": "path_to_small_few_shot_rag_results.csv",
        "graphrag": "path_to_small_few_shot_graphrag_results.csv",
    },
    "small_zero_shot": {
        "baseline": "path_to_small_zero_shot_baseline_results.csv",
        "rag": "path_to_small_zero_shot_rag_results.csv",
        "graphrag": "path_to_small_zero_shot_graphrag_results.csv",
    },
}


os.makedirs("your_results_path", exist_ok=True)
error_log_path = "results_path/error_log.log"
claim_mismatch_log = "results_path/claim_mismatches.log"


# Process each setting
for setting_name, files in file_mapping.items():
    baseline_file = files["baseline"]
    rag_file = files["rag"]
    graphrag_file = files["graphrag"]

    # Check all files exist
    if not all(os.path.exists(f) for f in [baseline_file, rag_file, graphrag_file]):
        print(f"Missing files for {setting_name}, skipping...", flush=True)
        continue

    output_filename = f"results_path_{setting_name}.csv"
    save_path = f"results_path/{output_filename}"

    print(f"\n{'='*100}")
    print(f"Processing setting: {setting_name}")
    print(f"{'='*100}")

    df_baseline = pd.read_csv(baseline_file)
    df_rag = pd.read_csv(rag_file)
    df_graphrag = pd.read_csv(graphrag_file)

    for df in [df_baseline, df_rag, df_graphrag]:
        df["claim"] = df["claim"].astype(str).str.strip()
        df["claim_norm"] = df["claim"].apply(normalize_claim)
        df["predicted_label"] = (
            df["predicted_label"].astype(str).str.strip().str.lower()
        )
        df["label"] = df["label"].astype(str).str.strip().str.lower()

    baseline_dict = {row["claim"]: row for _, row in df_baseline.iterrows()}
    rag_dict = {row["claim"]: row for _, row in df_rag.iterrows()}
    graphrag_dict = {row["claim"]: row for _, row in df_graphrag.iterrows()}

    common_claims = (
        set(baseline_dict.keys()) & set(rag_dict.keys()) & set(graphrag_dict.keys())
    )

    baseline_only = set(baseline_dict.keys()) - common_claims
    rag_only = set(rag_dict.keys()) - common_claims
    graphrag_only = set(graphrag_dict.keys()) - common_claims

    if baseline_only or rag_only or graphrag_only:
        with open(claim_mismatch_log, "a", encoding="utf-8") as f:
            f.write(f"\n{'='*70}\n")
            f.write(f"SETTING: {setting_name}\n")
            f.write(f"TIMESTAMP: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"{'='*70}\n")
            if baseline_only:
                f.write(
                    f"Claims only in Baseline ({len(baseline_only)}): {list(baseline_only)[:5]}\n"
                )
            if rag_only:
                f.write(f"Claims only in RAG ({len(rag_only)}): {list(rag_only)[:5]}\n")
            if graphrag_only:
                f.write(
                    f"Claims only in GraphRAG ({len(graphrag_only)}): {list(graphrag_only)[:5]}\n"
                )
            f.write(f"{'='*70}\n\n")
        print(
            f"Found {len(baseline_only)} Baseline-only, {len(rag_only)} RAG-only, {len(graphrag_only)} GraphRAG-only claims"
        )

    processed_claims, failed_claims, rows_in_output = load_processed_claims(save_path)

    total_claims = len(common_claims)
    print(f"Already processed successfully: {len(processed_claims)}")
    print(f"Previously failed (will retry): {len(failed_claims)}")
    print(
        f"Never attempted: {total_claims - len(processed_claims) - len(failed_claims)}"
    )
    print("-" * 100)

    if failed_claims:
        print(f"\n{'~'*100}")
        print(f"REPROCESSING {len(failed_claims)} PREVIOUSLY FAILED CLAIMS")
        print(f"{'~'*100}")

        reprocess_success = 0
        reprocess_still_failed = 0

        for claim_idx, claim in enumerate(sorted(failed_claims)):
            if (
                claim not in baseline_dict
                or claim not in rag_dict
                or claim not in graphrag_dict
            ):
                print(
                    f"Claim not found in all dataframes, skipping: {claim[:50]}",
                    flush=True,
                )
                continue

            baseline_row = baseline_dict[claim]
            rag_row = rag_dict[claim]
            graphrag_row = graphrag_dict[claim]

            baseline_label = baseline_row["predicted_label"]
            baseline_justification = baseline_row["predicted_justification"]

            rag_label = rag_row["predicted_label"]
            rag_justification = rag_row["predicted_justification"]

            graphrag_label = graphrag_row["predicted_label"]
            graphrag_justification = graphrag_row["predicted_justification"]

            full_prompt = PROMPT.format(
                claim=claim,
                baseline_label=baseline_label,
                baseline_justification=baseline_justification,
                rag_label=rag_label,
                rag_justification=rag_justification,
                graphrag_label=graphrag_label,
                graphrag_justification=graphrag_justification,
            )

            result, raw_response = process_claim_with_retry(
                full_prompt, claim, error_log_path, max_retries=2, retry_delay=2
            )

            update_failed_claim_in_csv(save_path, claim, result, raw_response)

            if result is not None:
                reprocess_success += 1
                processed_claims.add(claim)
                failed_claims.discard(claim)
                print(
                    f"✅ [{reprocess_success} recovered] Claim {claim_idx}/{len(failed_claims)-1}: "
                    f"1st: {result['ranking'][0]}",
                    flush=True,
                )
            else:
                reprocess_still_failed += 1
                print(
                    f"[{reprocess_still_failed} still failed] Claim {claim_idx}/{len(failed_claims)-1}: "
                    f"Stored raw output",
                    flush=True,
                )

            time.sleep(0.1)

        print(f"\n{'~'*100}")
        print(f"Reprocessing Summary:")
        print(f"  ✅ Recovered: {reprocess_success}")
        print(f"Still failed (raw output stored): {reprocess_still_failed}")
        print(f"{'~'*100}\n")

    newly_processed = 0
    skipped_count = 0
    error_count = 0

    for claim_idx, claim in enumerate(sorted(common_claims)):

        if claim in processed_claims or claim in failed_claims:
            skipped_count += 1
            continue

        baseline_row = baseline_dict[claim]
        rag_row = rag_dict[claim]
        graphrag_row = graphrag_dict[claim]

        baseline_label = baseline_row["predicted_label"]
        baseline_justification = baseline_row["predicted_justification"]

        rag_label = rag_row["predicted_label"]
        rag_justification = rag_row["predicted_justification"]

        graphrag_label = graphrag_row["predicted_label"]
        graphrag_justification = graphrag_row["predicted_justification"]

        if (
            pd.isna(baseline_justification)
            or pd.isna(rag_justification)
            or pd.isna(graphrag_justification)
        ):
            print(f"Claim {claim_idx}: Missing justification, skipping", flush=True)
            skipped_count += 1
            continue

        full_prompt = PROMPT.format(
            claim=claim,
            baseline_label=baseline_label,
            baseline_justification=baseline_justification,
            rag_label=rag_label,
            rag_justification=rag_justification,
            graphrag_label=graphrag_label,
            graphrag_justification=graphrag_justification,
        )

        result, raw_response = process_claim_with_retry(
            full_prompt, claim, error_log_path, max_retries=2, retry_delay=2
        )

        row_dict = {
            "claim": claim,
            "rank_1st": (
                result["ranking"][0] if result and len(result["ranking"]) > 0 else None
            ),
            "rank_2nd": (
                result["ranking"][1] if result and len(result["ranking"]) > 1 else None
            ),
            "rank_3rd": (
                result["ranking"][2] if result and len(result["ranking"]) > 2 else None
            ),
            "explanation": result.get("explanation", "") if result else None,
        }

        with open(save_path, "a", newline="", encoding="utf-8") as f:
            row_df = pd.DataFrame([row_dict])
            row_df.to_csv(f, header=f.tell() == 0, index=False)
            f.flush()
            os.fsync(f.fileno())

        if result is not None:
            processed_claims.add(claim)
        else:
            failed_claims.add(claim)
            error_count += 1

        newly_processed += 1

        if result is None:
            print(
                f"[{error_count} errors] Claim {claim_idx}: Failed - saved with NULL rankings and raw output",
                flush=True,
            )

        total_done = len(processed_claims)
        progress_pct = total_done / total_claims * 100
        print(
            f"[{newly_processed} new] Claim {claim_idx}/{total_claims-1} | "
            f"1st: {row_dict['rank_1st']} | "
            f"Progress: {total_done}/{total_claims} ({progress_pct:.1f}%)",
            flush=True,
        )

    print(f"\n{'─'*100}")
    print(f"Summary for {setting_name}:")
    print(f"Newly processed: {newly_processed}")
    print(f"Skipped (already done): {skipped_count}")
    print(f"Failed (saved with NULL and raw output): {error_count}")
    print(
        f"Total completed: {len(processed_claims)}/{total_claims} ({len(processed_claims)/total_claims*100:.1f}%)"
    )

    _, _, final_rows = load_processed_claims(save_path)
    print(f"Verified rows in output: {final_rows}")
    print(f"{'─'*100}\n")

print("=" * 100)
print(f"Check {error_log_path} for any parsing errors.")
print(f"Check {claim_mismatch_log} for any claim mismatches.")
print("=" * 100)
